{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":23812,"sourceType":"datasetVersion","datasetId":17810}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Kaggle-compatible training script for Chest X-Ray Images (Pneumonia) dataset\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-12-06T11:57:32.812211Z","iopub.execute_input":"2025-12-06T11:57:32.812840Z","iopub.status.idle":"2025-12-06T13:06:15.659634Z","shell.execute_reply.started":"2025-12-06T11:57:32.812802Z","shell.execute_reply":"2025-12-06T13:06:15.658958Z"}}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport joblib\n\n# Kaggle specific setup\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"Keras version:\", keras.__version__)\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Check for GPU\nprint(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n\n# Paths in Kaggle\nBASE_PATH = '/kaggle/input/chest-xray-pneumonia/chest_xray'\nTRAIN_PATH = os.path.join(BASE_PATH, 'train')\nVAL_PATH = os.path.join(BASE_PATH, 'val')\nTEST_PATH = os.path.join(BASE_PATH, 'test')\n\n# Create output directory\nOUTPUT_DIR = '/kaggle/working/models'\nos.makedirs(OUTPUT_DIR, exist_ok=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data Analysis Function","metadata":{}},{"cell_type":"code","source":"def load_and_analyze_data():\n    \"\"\"Load and analyze dataset\"\"\"\n    print(\"Analyzing dataset structure  \")\n    \n    # Count images in each directory\n    data_info = {}\n    for split in ['train', 'val', 'test']:\n        split_path = os.path.join(BASE_PATH, split)\n        if os.path.exists(split_path):\n            for class_name in ['NORMAL', 'PNEUMONIA']:\n                class_path = os.path.join(split_path, class_name)\n                if os.path.exists(class_path):\n                    num_images = len(os.listdir(class_path))\n                    data_info[f'{split}_{class_name}'] = num_images\n                    print(f\"{split}/{class_name}: {num_images} images\")\n    \n    return data_info","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data Generators Function","metadata":{}},{"cell_type":"code","source":"def create_data_generators():\n    \"\"\"Create data generators with augmentation\"\"\"\n    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n    \n    # Training data generator with augmentation\n    train_datagen = ImageDataGenerator(\n        rescale=1./255,\n        rotation_range=20,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.15,\n        zoom_range=0.15,\n        horizontal_flip=True,\n        fill_mode='nearest',\n        brightness_range=[0.8, 1.2]\n    )\n    \n    # Validation and test data generator (only rescaling)\n    val_test_datagen = ImageDataGenerator(rescale=1./255)\n    \n    # Create generators\n    train_generator = train_datagen.flow_from_directory(\n        TRAIN_PATH,\n        target_size=(224, 224),\n        batch_size=32,\n        class_mode='binary',\n        color_mode='rgb',\n        shuffle=True,\n        seed=42\n    )\n    \n    # For validation, if separate val directory exists\n    if os.path.exists(VAL_PATH) and len(os.listdir(VAL_PATH)) > 0:\n        validation_generator = val_test_datagen.flow_from_directory(\n            VAL_PATH,\n            target_size=(224, 224),\n            batch_size=32,\n            class_mode='binary',\n            color_mode='rgb',\n            shuffle=False\n        )\n    else:\n        # If no validation directory, split training data\n        print(\"No validation directory found. Splitting training data \")\n        total_samples = train_generator.samples\n        split_index = int(total_samples * 0.8)\n        \n        # Create validation generator from training data\n        # Note: This is simplified. In practice, you'd want to properly split the data\n        validation_generator = val_test_datagen.flow_from_directory(\n            TRAIN_PATH,\n            target_size=(224, 224),\n            batch_size=32,\n            class_mode='binary',\n            color_mode='rgb',\n            shuffle=False,\n            subset='validation'  # Requires validation_split parameter\n        )\n    \n    test_generator = val_test_datagen.flow_from_directory(\n        TEST_PATH,\n        target_size=(224, 224),\n        batch_size=32,\n        class_mode='binary',\n        color_mode='rgb',\n        shuffle=False\n    )\n    \n    return train_generator, validation_generator, test_generator","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Building Function","metadata":{}},{"cell_type":"code","source":"def build_advanced_model():\n    \"\"\"Build an advanced CNN model\"\"\"\n    model = keras.Sequential([\n        # First convolutional block\n        layers.Conv2D(64, (3, 3), activation='relu', padding='same', \n                      input_shape=(224, 224, 3)),\n        layers.BatchNormalization(),\n        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D((2, 2)),\n        layers.Dropout(0.25),\n        \n        # Second convolutional block\n        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D((2, 2)),\n        layers.Dropout(0.25),\n        \n        # Third convolutional block\n        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D((2, 2)),\n        layers.Dropout(0.25),\n        \n        # Fourth convolutional block\n        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D((2, 2)),\n        layers.Dropout(0.25),\n        \n        # Flatten and dense layers\n        layers.Flatten(),\n        layers.Dense(512, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.5),\n        \n        layers.Dense(256, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.5),\n        \n        # Output layer\n        layers.Dense(1, activation='sigmoid')\n    ])\n    \n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Training Function","metadata":{}},{"cell_type":"code","source":"def train_model():\n    \"\"\"Train the model with callbacks\"\"\"\n    # Load data\n    print(\"Loading data...\")\n    train_gen, val_gen, test_gen = create_data_generators()\n    \n    # Calculate class weights\n    from sklearn.utils.class_weight import compute_class_weight\n    classes = train_gen.classes\n    class_weights = compute_class_weight(\n        'balanced',\n        classes=np.unique(classes),\n        y=classes\n    )\n    class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n    print(f\"Class weights: {class_weight_dict}\")\n    \n    # Build model\n    print(\"Building model...\")\n    model = build_advanced_model()\n    \n    # Compile model\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n        loss='binary_crossentropy',\n        metrics=[\n            'accuracy',\n            keras.metrics.Precision(name='precision'),\n            keras.metrics.Recall(name='recall'),\n            keras.metrics.AUC(name='auc')\n        ]\n    )\n    \n    print(\"Model summary:\")\n    model.summary()\n    \n    # Callbacks\n    callbacks = [\n        keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=15,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        keras.callbacks.ModelCheckpoint(\n            os.path.join(OUTPUT_DIR, 'best_model_kaggle.h5'),\n            monitor='val_loss',\n            save_best_only=True,\n            verbose=1\n        ),\n        keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=7,\n            min_lr=0.000001,\n            verbose=1\n        ),\n        keras.callbacks.TensorBoard(\n            log_dir=os.path.join(OUTPUT_DIR, 'logs'),\n            histogram_freq=1\n        )\n    ]\n    \n    # Train model\n    print(\"Training model...\")\n    history = model.fit(\n        train_gen,\n        epochs=50,\n        validation_data=val_gen,\n        callbacks=callbacks,\n        class_weight=class_weight_dict,\n        verbose=1\n    )\n    \n    return model, history, test_gen","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Evaluation Function","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model, test_gen):\n    \"\"\"Evaluate model on test set\"\"\"\n    print(\"\\nEvaluating on test set...\")\n    \n    # Get predictions\n    test_gen.reset()\n    y_pred_proba = model.predict(test_gen, verbose=1)\n    y_pred = (y_pred_proba > 0.5).astype(int)\n    \n    # Get true labels\n    y_true = test_gen.classes\n    \n    # Calculate metrics\n    from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"TEST SET EVALUATION\")\n    print(\"=\"*50)\n    \n    # Classification report\n    print(\"\\nClassification Report:\")\n    print(classification_report(y_true, y_pred, \n                                target_names=['NORMAL', 'PNEUMONIA']))\n    \n    # Confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    print(\"\\nConfusion Matrix:\")\n    print(cm)\n    \n    # ROC-AUC\n    roc_auc = roc_auc_score(y_true, y_pred_proba)\n    print(f\"\\nROC-AUC Score: {roc_auc:.4f}\")\n    \n    # Plot confusion matrix\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=['NORMAL', 'PNEUMONIA'],\n                yticklabels=['NORMAL', 'PNEUMONIA'])\n    plt.title('Confusion Matrix')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.savefig(os.path.join(OUTPUT_DIR, 'confusion_matrix.png'), dpi=100)\n    plt.show()\n    \n    return y_true, y_pred, y_pred_proba","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training History Plotting Function","metadata":{}},{"cell_type":"code","source":"def plot_training_history(history):\n    \"\"\"Plot training history\"\"\"\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    \n    metrics = ['loss', 'accuracy', 'precision', 'recall', 'auc']\n    \n    for i, metric in enumerate(metrics):\n        ax = axes[i // 3, i % 3]\n        ax.plot(history.history[metric], label=f'Training {metric}')\n        ax.plot(history.history[f'val_{metric}'], label=f'Validation {metric}')\n        ax.set_title(f'Model {metric}')\n        ax.set_xlabel('Epoch')\n        ax.set_ylabel(metric)\n        ax.legend()\n        ax.grid(True)\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, 'training_history.png'), dpi=100)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T17:41:45.058109Z","iopub.execute_input":"2025-12-06T17:41:45.059230Z","iopub.status.idle":"2025-12-06T17:41:45.070295Z","shell.execute_reply.started":"2025-12-06T17:41:45.059195Z","shell.execute_reply":"2025-12-06T17:41:45.069435Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"### Model Saving Function","metadata":{}},{"cell_type":"code","source":"def save_model_for_production(model, test_gen):\n    \"\"\"Save model for production use\"\"\"\n    print(\"\\nSaving model for production...\")\n    \n    # Save Keras model\n    model.save(os.path.join(OUTPUT_DIR, 'pneumonia_model_final.h5'))\n    \n    # Save with joblib (including preprocessing info)\n    pipeline = {\n        'model': model,\n        'img_size': (224, 224),\n        'class_names': ['NORMAL', 'PNEUMONIA'],\n        'class_indices': test_gen.class_indices,\n        'threshold': 0.5,\n        'version': '2.0.0',\n        'trained_on': 'kaggle'\n    }\n    \n    joblib.dump(pipeline, os.path.join(OUTPUT_DIR, 'pneumonia_pipeline.pkl'))\n    \n    # Save model architecture as JSON\n    model_json = model.to_json()\n    with open(os.path.join(OUTPUT_DIR, 'model_architecture.json'), 'w') as f:\n        f.write(model_json)\n    \n    print(f\"Models saved to {OUTPUT_DIR}\")\n    print(f\"Files created:\")\n    print(f\"  - pneumonia_model_final.h5\")\n    print(f\"  - pneumonia_pipeline.pkl\")\n    print(f\"  - model_architecture.json\")\n    print(f\"  - training_history.png\")\n    print(f\"  - confusion_matrix.png\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Submission File Creation Function","metadata":{}},{"cell_type":"code","source":"def create_submission_file(model, test_gen):\n    \"\"\"Create Kaggle submission file if in competition\"\"\"\n    # This is optional and depends on if you're in a Kaggle competition\n    submission_dir = '/kaggle/working/submission'\n    os.makedirs(submission_dir, exist_ok=True)\n    \n    print(\"\\nCreating sample predictions for verification...\")\n    \n    # Get a few sample predictions\n    test_gen.reset()\n    sample_batch = next(test_gen)\n    sample_images, sample_labels = sample_batch\n    \n    predictions = model.predict(sample_images[:5])\n    \n    # Create a simple verification file\n    verification_df = pd.DataFrame({\n        'image_index': range(5),\n        'true_label': sample_labels[:5],\n        'predicted_prob': predictions.flatten()[:5],\n        'predicted_label': (predictions.flatten()[:5] > 0.5).astype(int)\n    })\n    \n    verification_df.to_csv(os.path.join(submission_dir, 'sample_predictions.csv'), index=False)\n    print(\"Sample predictions saved for verification\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Main Execution Function and Entry Point","metadata":{}},{"cell_type":"code","source":"def main():\n    \"\"\"Main training function\"\"\"\n    print(\"Starting Kaggle training pipeline...\")\n    print(\"=\"*60)\n    \n    # Step 1: Analyze data\n    data_info = load_and_analyze_data()\n    \n    # Step 2: Train model\n    model, history, test_gen = train_model()\n    \n    # Step 3: Evaluate model\n    evaluate_model(model, test_gen)\n    \n    # Step 4: Plot training history\n    plot_training_history(history)\n    \n    # Step 5: Save model for production\n    save_model_for_production(model, test_gen)\n    \n    # Step 6: Create submission file (for Kaggle competition if applicable)\n    create_submission_file(model, test_gen)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"Training completed successfully!\")\n    print(f\"Model files are saved in: {OUTPUT_DIR}\")\n    print(\"=\"*60)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}